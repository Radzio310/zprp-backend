# app/agent_chat.py

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import List, Literal, Optional
from sqlalchemy import select
import json
import math

from app.db import database, agent_document_chunks
from app.groq_client import groq_chat_completion


class ChatMessage(BaseModel):
    role: Literal["system", "user", "assistant"]
    content: str


class AgentQueryRequest(BaseModel):
    messages: List[ChatMessage]
    model: Optional[str] = "llama-3.1-8b-instant"
    temperature: float = 0.2
    max_tokens: int = 1024
    max_context_chunks: int = 8


class AgentQueryResponse(BaseModel):
    reply: str


router = APIRouter(prefix="/agent", tags=["agent"])


def cosine_similarity(a: List[float], b: List[float]) -> float:
    if not a or not b or len(a) != len(b):
        return 0.0
    dot = sum(x * y for x, y in zip(a, b))
    na = math.sqrt(sum(x * x for x in a))
    nb = math.sqrt(sum(x * x for x in b))
    if na == 0 or nb == 0:
        return 0.0
    return dot / (na * nb)


async def embed_query(text: str) -> List[float]:
    """
    Embedding zapytania ‚Äì u≈ºywamy tego samego ‚Äûembedding hacka‚Äù
    co w app.agent_docs.simple_embed, ≈ºeby przestrze≈Ñ by≈Ça sp√≥jna.
    """
    from app.agent_docs import simple_embed  # unikamy duplikacji kodu

    vecs = await simple_embed([text])
    return vecs[0]


@router.post("/query", response_model=AgentQueryResponse)
async def agent_query(payload: AgentQueryRequest):
    if not payload.messages:
        raise HTTPException(status_code=400, detail="Brak wiadomo≈õci")

    # znajd≈∫ ostatniƒÖ wiadomo≈õƒá u≈ºytkownika
    user_messages = [m for m in payload.messages if m.role == "user"]
    if not user_messages:
        raise HTTPException(
            status_code=400, detail="Brak wiadomo≈õci u≈ºytkownika w historii"
        )
    last_user_msg = user_messages[-1]

    print("[agent_query] Ostatnia wiadomo≈õƒá usera:", last_user_msg.content)

    # embedding zapytania
    query_vec = await embed_query(last_user_msg.content)
    print("[agent_query] D≈Çugo≈õƒá wektora zapytania:", len(query_vec))
    print("[agent_query] Pierwsze kilka warto≈õci zapytania:", query_vec[:5])

    # pobierz wszystkie chunki (v1 ‚Äì prosty wariant, mo≈ºna potem dodaƒá filtr po dokumencie)
    q = select(agent_document_chunks)
    rows = await database.fetch_all(q)

    print("[agent_query] Liczba chunk√≥w w bazie:", len(rows))

    # je≈õli Bazyli nie ma w og√≥le wiedzy ‚Äì nie pytamy Groqa, tylko m√≥wimy wprost
    if len(rows) == 0:
        print("[agent_query] Brak jakichkolwiek chunk√≥w ‚Äì Bazyli jest 'na g≈Çodno' ü§ñ")
        return AgentQueryResponse(
            reply=(
                "Nie mam jeszcze ≈ºadnych dokument√≥w w pamiƒôci, wiƒôc nie mogƒô "
                "odpowiedzieƒá na to pytanie. Wejd≈∫ w panel Bazylego i wgraj "
                "przynajmniej jeden plik PDF, z kt√≥rego mogƒô siƒô uczyƒá."
            )
        )

    scored: List[tuple[float, dict]] = []
    for row in rows:
        try:
            emb = json.loads(row["embedding"])
            sim = cosine_similarity(query_vec, [float(x) for x in emb])
        except Exception as e:
            print("[agent_query] B≈ÇƒÖd przy liczeniu similarity:", e)
            sim = 0.0
        scored.append((sim, dict(row)))

    # posortuj po similarity malejƒÖco
    scored.sort(key=lambda x: x[0], reverse=True)

    debug_top_n = min(payload.max_context_chunks, len(scored))
    print(f"[agent_query] TOP {debug_top_n} chunk√≥w wg similarity:")
    for i, (sim, r) in enumerate(scored[:debug_top_n]):
        snippet = r["content"][:150].replace("\n", " ")
        print(
            f"  #{i} sim={sim:.4f}, doc_id={r['document_id']}, "
            f"chunk_index={r['chunk_index']}, snippet='{snippet}'"
        )

    # do KONTEKSTU bierzemy tylko te z similarity > 0
    top = [r for (s, r) in scored[: payload.max_context_chunks] if s > 0]

    print(f"[agent_query] Liczba chunk√≥w z sim>0 u≈ºytych w kontek≈õcie: {len(top)}")

    # je≈õli nie znale≈∫li≈õmy ≈ºadnego sensownego dopasowania ‚Äì NIE pytamy Groqa
    if not top:
        print(
            "[agent_query] Brak chunk√≥w z dodatniƒÖ similarity ‚Äì zwracam 'nie wiem' "
            "bez odpytywania modelu."
        )
        return AgentQueryResponse(
            reply=(
                "Przejrza≈Çem wszystkie swoje dokumenty, ale nie znalaz≈Çem w nich "
                "informacji, kt√≥re pasowa≈Çyby do tego pytania. "
                "Spr√≥buj sformu≈Çowaƒá je inaczej albo wgraj PDF, kt√≥ry to opisuje."
            )
        )

    # zlep kontekst z chunk√≥w
    context_text = "\n\n---\n\n".join(
        f"[Fragment #{r['chunk_index']}] (doc_id={r['document_id']})\n{r['content']}"
        for r in top
    )

    # dla bezpiecze≈Ñstwa nie logujmy ca≈Çego kontekstu je≈õli jest gigantyczny
    if context_text:
        print("[agent_query] KONTEKST (poczƒÖtek):")
        print(context_text[:2000])

    system_prompt = (
        "Jeste≈õ asystentem Bazyli, kt√≥ry odpowiada wy≈ÇƒÖcznie w oparciu o podany kontekst.\n"
        "Je≈õli czego≈õ nie ma w kontek≈õcie, jasno powiedz, ≈ºe nie wiesz zamiast zmy≈õlaƒá.\n"
        "Kontekst mo≈ºe byƒá po polsku, odpowiadaj po polsku, chyba ≈ºe u≈ºytkownik wyra≈∫nie prosi inaczej.\n"
    )

    # zbuduj historiƒô dla Groqa:
    groq_messages: List[dict] = [{"role": "system", "content": system_prompt}]

    # KONTEKST z PDF-√≥w jako osobna wiadomo≈õƒá systemowa
    groq_messages.append(
        {
            "role": "system",
            "content": (
                "Kontekst do wykorzystania (fragmenty dokument√≥w u≈ºytkownika):\n\n"
                f"{context_text}"
            ),
        }
    )

    # dodaj wszystkie dotychczasowe wiadomo≈õci u≈ºytkownika / asystenta,
    # ale bez wcze≈õniejszych system√≥w, bo je nadpisali≈õmy:
    for m in payload.messages:
        if m.role in ("user", "assistant"):
            groq_messages.append({"role": m.role, "content": m.content})

    print("[agent_query] Liczba wiadomo≈õci wysy≈Çanych do Groqa:", len(groq_messages))

    reply = await groq_chat_completion(
        messages=groq_messages,
        model=payload.model or "llama-3.1-8b-instant",
        temperature=payload.temperature,
        max_tokens=payload.max_tokens,
    )

    print("[agent_query] Odpowied≈∫ z Groqa (poczƒÖtek):", reply[:500])

    return AgentQueryResponse(reply=reply)
